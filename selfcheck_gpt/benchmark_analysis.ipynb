{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39bc5154",
   "metadata": {},
   "source": [
    "#### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed91855d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from sklearn.metrics import precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b02d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dict = {\n",
    "    \"GPT 4o Mini\": \"data/scores_gpt4o_mini.json\",\n",
    "    \"Qwen3 4B Instruct\": \"data/scores_qwen3_4b_instruct.json\",\n",
    "    \"BERTScore\": \"data/scores_bertscore.json\"\n",
    "}\n",
    "\n",
    "def load_scores(scores_dict):\n",
    "    scores = {}\n",
    "    \n",
    "    for model in scores_dict:\n",
    "        with open(scores_dict[model]) as f:\n",
    "            scores[model] = json.load(f)\n",
    "    return scores\n",
    "\n",
    "scores = load_scores(scores_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3c2d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/dataset.json\", \"r\") as f:\n",
    "    dataset = json.loads(f.read())\n",
    "\n",
    "print(f\"The length of the dataset: {len(dataset)}\")\n",
    "print(\"The keys of each sample:\", list(dataset[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09130058",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [str(sample['wiki_bio_test_idx']) for sample in dataset] \n",
    "labels_mapping = {\n",
    "    'accurate': 0.0,\n",
    "    'minor_inaccurate': 0.5,\n",
    "    'major_inaccurate': 1.0,\n",
    "}\n",
    "human_labels_detect_true = {}\n",
    "human_labels_detect_false = {}\n",
    "human_labels_detect_hard_false = {}\n",
    "human_labels_passage_mean = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    sample = dataset[i]\n",
    "    idx = sample['wiki_bio_test_idx']\n",
    "    raw_label = np.array([labels_mapping[x] for x in sample['annotation']])\n",
    "    \n",
    "    human_labels_detect_true[idx] = (raw_label < 0.499).astype(np.int32).tolist()\n",
    "    human_labels_detect_false[idx] = (raw_label > 0.499).astype(np.int32).tolist()\n",
    "    if np.mean(raw_label) < 0.99:\n",
    "        human_labels_detect_hard_false[idx] = (raw_label > 0.99).astype(np.int32).tolist()\n",
    "    human_labels_passage_mean.append(np.mean(raw_label))\n",
    "\n",
    "print(\"Length of true human labels:\", len(human_labels_detect_true))\n",
    "print(\"Length of false human labels:\", len(human_labels_detect_false))\n",
    "print(\"Length of hard false human labels\", len(human_labels_detect_hard_false))\n",
    "print(\"Length of passage mean human labels:\", len(human_labels_passage_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f222e2e",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcd2598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_baseline(human_label):\n",
    "    arr = []\n",
    "    \n",
    "    for v in human_label.values():\n",
    "        arr.extend(v)\n",
    "    return np.mean(arr)\n",
    "\n",
    "random_baseline_true = get_random_baseline(human_labels_detect_true)\n",
    "random_baseline_false = get_random_baseline(human_labels_detect_false)\n",
    "random_baseline_false_hard = get_random_baseline(human_labels_detect_hard_false)\n",
    "\n",
    "print(\"Random baseline true:\", np.round(random_baseline_true, 3))\n",
    "print(\"Random baseline false:\", np.round(random_baseline_false, 3))\n",
    "print(\"Random baseline hard false:\", np.round(random_baseline_false_hard, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab706c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll_labels(labels, indices):\n",
    "    unrolled_labels = []\n",
    "    \n",
    "    for idx in indices:\n",
    "        unrolled_labels.extend(labels[idx])\n",
    "    return unrolled_labels\n",
    "\n",
    "def get_pr_with_human_labels(preds, human_labels, pos_label=1, oneminus_pred=False):\n",
    "    indices = [k for k in human_labels.keys()]\n",
    "    flatten_labels = unroll_labels(human_labels, indices)\n",
    "    flatten_preds = unroll_labels(preds, indices)\n",
    "\n",
    "    if oneminus_pred:\n",
    "        flatten_preds = [1.0-x for x in flatten_preds]        \n",
    "    assert(len(flatten_preds) == len(flatten_labels))\n",
    "    \n",
    "    p, r, threshold = precision_recall_curve(flatten_labels, flatten_preds, pos_label=pos_label)\n",
    "    return p, r, threshold\n",
    "\n",
    "def get_auc(preds, human_labels, oneminus_pred=False):\n",
    "    prec, rec, threshold = get_pr_with_human_labels(preds, human_labels, pos_label=1, oneminus_pred=oneminus_pred)\n",
    "    return np.round(auc(prec, rec) * 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac994bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_passage_metrics(preds, human_labels_passage_mean):\n",
    "    passage_mean_preds = [np.mean(preds[id]) for id in preds]\n",
    "    pearsonr = stats.pearsonr(passage_mean_preds, human_labels_passage_mean)\n",
    "    spearmanr = stats.spearmanr(passage_mean_preds, human_labels_passage_mean)\n",
    "    return {\"Pearson\": pearsonr[0], \"Spearman\": spearmanr[0]}\n",
    "\n",
    "def get_result(name, preds, human_labels_passage_mean):\n",
    "    passage_metrics = get_passage_metrics(preds, human_labels_passage_mean)\n",
    "    \n",
    "    row = [\n",
    "        name,\n",
    "        np.round(get_auc(preds, human_labels_detect_false), 3),\n",
    "        np.round(get_auc(preds, human_labels_detect_hard_false), 3),\n",
    "        np.round(get_auc(preds, human_labels_detect_true), 3),\n",
    "        np.round(passage_metrics['Pearson'], 3),\n",
    "        np.round(passage_metrics['Spearman'], 3),\n",
    "    ]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    result = pd.DataFrame(columns=[\"Method\", \"NonFact\", \"Hard NonFact\", \"Factual\", \"Pearson\", \"Spearman\"])\n",
    "\n",
    "    random_baseline_row = [\n",
    "        \"Random Baseline\",\n",
    "        np.round(random_baseline_false * 100, 3),\n",
    "        np.round(random_baseline_false_hard * 100, 3),\n",
    "        np.round(random_baseline_false * 100, 3),\n",
    "        \"\", \n",
    "        \"\",\n",
    "    ]\n",
    "    result.loc[len(result)] = random_baseline_row\n",
    "\n",
    "    for model_name, preds in scores.items():\n",
    "        result.loc[len(result)] = get_result(model_name, preds, human_labels_passage_mean)\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base-11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
